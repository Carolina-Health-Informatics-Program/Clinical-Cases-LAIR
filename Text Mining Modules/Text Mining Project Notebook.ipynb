{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the modules! This is the project notebook for the text mining track. This will give you a chance to apply your skills. In this project, you will select your dataset, create your own analysis using the skills you've learned in the previous module, and write up your findings. This will give you a chance to get your hands dirty using real healthcare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a list of carefully curated datasets for you to use. Please read about your selected dataset and run the associated code cell for your dataset. \n",
    "\n",
    "However, if you prefer to use your own dataset go ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Review Dataset (Druglib.com) Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides patient reviews on specific drugs along with related conditions. Reviews and ratings are grouped into reports on the three aspects benefits, side effects and overall comment.  \n",
    "The dataset provides patient reviews on specific drugs along with related conditions. Furthermore, reviews are grouped into reports on the three aspects benefits, side effects and overall comment. Additionally, ratings are available concerning overall satisfaction as well as a 5 step side effect rating and a 5 step effectiveness rating. The data was obtained by crawling online pharmaceutical review sites. The intention was to study  \n",
    "  \n",
    "(1) sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects,  \n",
    "(2) the transferability of models among domains, i.e. conditions, and  \n",
    "(3) the transferability of models among different data sources (see 'Drug Review Dataset (Drugs.com)').\n",
    "  \n",
    "The dataset includes a separate testing dataset (drugLibTest_raw.csv). Use drugLibTrain_raw.csv for building a prediction model. Then measure performance metrics (e.g., accuracy, precision, recall, etc) using drugLibTest_raw.csv file. \n",
    "\n",
    "*Source: Felix Gräßer, Surya Kallumadi, Hagen Malberg, and Sebastian Zaunseder. 2018. Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning. In Proceedings of the 2018 International Conference on Digital Health (DH '18). ACM, New York, NY, USA, 121-125.*  \n",
    "  \n",
    "Visit https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Druglib.com%29 for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:04:13.392523Z",
     "start_time": "2020-06-22T02:04:13.368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code reading in the training dataset\n",
    "import pandas as pd\n",
    "druglib_train = pd.read_csv('drugLibTrain_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Druglib.com Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. urlDrugName (categorical): name of drug  \n",
    "2. condition (categorical): name of condition  \n",
    "3. benefitsReview (text): patient on benefits  \n",
    "4. sideEffectsReview (text): patient on side effects  \n",
    "5. commentsReview (text): overall patient comment  \n",
    "6. rating (numerical): 10 star patient rating  \n",
    "7. sideEffects (categorical): 5 step side effect rating  \n",
    "8. effectiveness (categorical): 5 step effectiveness rating  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Review Dataset (Drugs.com) Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides patient reviews on specific drugs along with related conditions and a 10 star patient rating reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. The intention was to study  \n",
    "  \n",
    "(1) sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects,  \n",
    "(2) the transferability of models among domains, i.e. conditions, and  \n",
    "(3) the transferability of models among different data sources (see 'Drug Review Dataset (Druglib.com)').  \n",
    "  \n",
    "The dataset includes a separate testing dataset (drugsComTest_raw.csv). Use drugsComTrain_raw.csv for building a prediction model. Then measure performance metrics (e.g., accuracy, precision, recall, etc) using drugsComTest_raw.csv file.  \n",
    "\n",
    "*Source: Felix Gräßer, Surya Kallumadi, Hagen Malberg, and Sebastian Zaunseder. 2018. Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning. In Proceedings of the 2018 International Conference on Digital Health (DH '18). ACM, New York, NY, USA, 121-125.  \n",
    "  \n",
    "Visit https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T01:54:14.446320Z",
     "start_time": "2020-06-22T01:54:14.413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code reading in the training dataset\n",
    "\n",
    "import pandas as pd\n",
    "drugs_train = pd.read_csv('drugsComTrain_raw.csv')\n",
    "\n",
    "drugs_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drugs.com Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. drugName (categorical): name of drug  \n",
    "2. condition (categorical): name of condition  \n",
    "3. review (text): patient review  \n",
    "4. rating (numerical): 10 star patient rating\n",
    "5. date (date): date of review entry\n",
    "6. usefulCount (numerical): number of users who found review useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was collected in 2015 using Twitter API. This dataset contains health news from more than 15 major health news agencies such as BBC, CNN, and NYT.  \n",
    "Each file is related to one Twitter account of a news agency. For example, bbchealth.txt is related to BBC health news. Each line contains tweet id|date and time|tweet. The separator is '|'. This text data has been used to evaluate the performance of topic models on short text data. However, it can be used for other tasks such as clustering.  \n",
    "  \n",
    "*Source: Karami, A., Gangopadhyay, A., Zhou, B., & Kharrazi, H. (2017). Fuzzy approach topic discovery in health and medical corpora. International Journal of Fuzzy Systems, 1-12.  \n",
    "  \n",
    "Visit https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T01:05:59.135567Z",
     "start_time": "2020-06-22T01:05:58.962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code reading in the 'cnnhealth.txt'\n",
    "file = open('cnnhealth.txt', encoding='utf-8') # open the original txt file\n",
    "\n",
    "data = {}\n",
    "\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line_elements = line.split('|') # separate four elements (tweet_id, date and tweet) in each line.\n",
    "    tweet_id = line_elements[0] # 'tweet_id' is the first element in each line.\n",
    "    time = line_elements[1] # 'date' is the second element in each line.\n",
    "    tweet = line_elements[2] # 'tweet' is the fourth element in each line.\n",
    "    data.setdefault(tweet_id,{}).setdefault(date, tweet) # see below\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a list of functions you may find helpful to use in the project. These functions are all based on the modules. You can simply pick up ones in \n",
    "\n",
    "Feel free to refer back to the module. Maybe put your project code and module code side by side if you need. Also to develop your own code is encouraged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\t\t\t<body>\n",
       "\t\t\t\t<h1>Help functions</h1>\n",
       "\t\t\t\t<div>\n",
       "                    <h2>Data Preprocessing Function</h2>\n",
       "                    <h3><code>remove_special_characters(text, remove_digits=False)</code></h3>\n",
       "                    <ul>\n",
       "                        Remove characters except a-zA-Z0-9 and blank space\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>text: str, the string text to remove special characters</li>\n",
       "                                <li>remove_digits: bool, if True the digits will be removed. Default is False</li>\n",
       "                            </ul>\n",
       "                        <li>:return: str, preprocessed text</li>                \n",
       "                    </ul>\n",
       "                    <h3><code>remove_stopwords(text, is_lower_case=False)</code></h3>\n",
       "                    <ul>\n",
       "                        Remove stopwords\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>text: str, the string text to remvoe stopwords</li>\n",
       "                                <li>is_lower_case: bool, if True the characters will be lowercased. Default is False</li>\n",
       "                            </ul>\n",
       "                        <li>:return: str, preprocessed text</li>                \n",
       "                    </ul>\n",
       "                    <h3><code>simple_stemmer(text)</code></h3>\n",
       "                    <ul>\n",
       "                        Stem string\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>text: str, the string text to stem</li>\n",
       "                            </ul>\n",
       "                        <li>:return: str, preprocessed text</li>                \n",
       "                    </ul>\n",
       "                    <h3><code>lemmatize_text(text)</code></h3>\n",
       "                    <ul>\n",
       "                        Lemmatize string\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>text: str, the string text to lemmatize</li>\n",
       "                            </ul>\n",
       "                        <li>:return: str, preprocessed text</li>                \n",
       "                    </ul>\n",
       "                    <h3><code>expand_contractions(text)</code></h3>\n",
       "                    <ul>\n",
       "                        expand contractions in text string\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>text: str, the string text to expand contractions, e.g., I'm - I am</li>\n",
       "                            </ul>\n",
       "                        <li>:return: str, preprocessed text</li>                \n",
       "                    </ul>\n",
       "                </div>\n",
       "\t\t\t\t\n",
       "\t\t\t\t\n",
       "                <div>\n",
       "\t\t\t\t\t<h2>Data Exploration Function</h2>\n",
       "\t\t\t\t\t\n",
       "                    <h3><code>freq_ngram(document, N = 1, allgram = False, top_count = 20)</code></h3>\n",
       "                    <ul>\n",
       "                        Get N-gram frequency\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>document: Series or list, the Series or list of the string text. \n",
       "\t\t\t\t\t\t\t\t\t<br>You can sumply put the string column to this part such as df[\"text\"] <br>\n",
       "\t\t\t\t\t\t\t\t</li>\n",
       "                                <li>N: int, type of gram you want to look at, bi-gram then use N=2</li>\n",
       "                                <li>allgram: bool, if True, all types of n-gram will be counted, default is False. <br>e.g., if allgram=3 \n",
       "                                    the uni-gram, bi-gram and tri-gram will all be counted</br></li>\n",
       "                            </ul>\n",
       "                        <li>:return: dataframe, datafame with word, count and frequency among all words as column name</li>                \n",
       "                    </ul>\n",
       "                    \n",
       "                    <h3><code>viz_ngram_freq(df, figsize = (8,8))</code></h3>\n",
       "                    <ul>\n",
       "                        Using barchart to plot the term frequency after using freq_ngram()\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>df: dataframe, the table that contains word and corresponding counts</li>\n",
       "                                <li>figsize: tuple, (width, height) size of the figure, default is (8,8)</li>\n",
       "                            </ul>\n",
       "                        </li>              \n",
       "                    </ul>\n",
       "\n",
       "                </div>\n",
       "\n",
       "                <div>\n",
       "                    <h2>Information Extraction</h2>\n",
       "\t\t\t\t\t<p>\n",
       "\t\t\t\t\t\tThis class provides you a handle to extract information from text. \n",
       "\t\t\t\t\t\tCode example: <br>\n",
       "\t\t\t\t\t\t<br>\n",
       "\t\t\t\t\t\t\t<code>extractor = ExtactInfo()</code><br>\n",
       "\t\t\t\t\t\t\t<code>extractor. start_extract(document)</code><br>\n",
       "\t\t\t\t\t\t\t<code>postag = extractor.get_postag()</code><br>\n",
       "\t\t\t\t\t\t\t<code>noun_phrase = extractor.get_noun_phrase()</code><br>\n",
       "\t\t\t\t\t\t<br>\n",
       "\t\t\t\t\t\tThe following funcitons should be used in this way: <br>\n",
       "\t\t\t\t\t\t<code>extractor.somefunction()</code><br>\n",
       "\t\t\t\t\t\t\n",
       "\t\t\t\t\t</p>\n",
       "                    <h3><code>start_extract(document)</code></h3>\n",
       "                    <ul>\n",
       "                        Feed the extractor with the document. This function should be used prior to any of the following function. \n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>documnet: Series or list, the Series or list of the string text. <br>You can sumply put the string column to this part such as df[\"text\"] <br></li>\n",
       "                             </ul>             \n",
       "\t\t\t\t\t\t</li> \n",
       "\t\t\t\t\t</ul>\n",
       "                    \n",
       "                    <h3><code>get_postag(postagtype = \"univ\")</code></h3>\n",
       "                    <ul>\n",
       "                        Compute similarity between two documents\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>postagtype: str, the types of pos tags to use. It can be \"univ\" (Universal postags) or \"penn\" (Penn treebank postags). default is univ. \n",
       "\t\t\t\t\t\t\t\t</li>\n",
       "                            </ul>\n",
       "                        </li>\n",
       "\t\t\t\t\t\t<li>:return: dataframe, a table that contain pos tags count of each document</li>  \t\t\t\t\t\t\n",
       "                    </ul>\n",
       "\t\t\t\t\t<h3><code>get_noun_phrase()</code></h3>\n",
       "                    <ul>\n",
       "                        Get noun phrases in each document\n",
       "\t\t\t\t\t\t<li>:return: list, will return a list of the noun phrase list of each document</li>  \t\t\t\t\t\t\n",
       "                    </ul>\n",
       "                </div>\t\t\t\t\n",
       "\n",
       "                <div>\n",
       "                    <h2>Vector Representation of Text</h2>\n",
       "                    <h3><code>create_bow_matrix(document, tfidf = True)</code></h3>\n",
       "                    <ul>\n",
       "                        Create bag-of-word vectors for text\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>documnet: Series or list, the Series or list of the string text. <br>You can sumply put the string column to this part such as df[\"text\"] <br></li>\n",
       "                                <li>tfidf: bool, if True, the vector will be weighted using tfidf method. Default is True.</li>\n",
       "                             </ul>\n",
       "                        <li>:return: dataframe, a table where each row is a document and each column is a word. The value is the tfidf weighted value </li>                \n",
       "                    </ul>\n",
       "                    \n",
       "                    <h3><code>get_text_similarity(doc_a, doc_b, method = \"cosine\")</code></h3>\n",
       "                    <ul>\n",
       "                        Compute similarity between two documents\n",
       "                        <li>:param \n",
       "                            <ul>\n",
       "                                <li>doc_a, doc_b: Series, list or string, two texts you want to compare. \n",
       "\t\t\t\t\t\t\t\t\t<br>e.g., you can input [\"I love puppies\", \"I am not a fan of tomato\"], [\"Puppies grow quickly after birth\", \"I love basketball\"]\n",
       "\t\t\t\t\t\t\t\t\t<br>you can also input\"I love puppies. I am not a fan of tomato\", \"Puppies grow quickly after birth. I love basketball\"\n",
       "\t\t\t\t\t\t\t\t</li>\n",
       "                                <li>method: str, \"cosine\" or \"euclidean\". Default is cosine</li>\n",
       "                            </ul>\n",
       "                        </li>\n",
       "\t\t\t\t\t\t<li>:return: dataframe, a table that contain pairwise similarity score of the input text</li>  \t\t\t\t\t\t\n",
       "                    </ul>\n",
       "                </div>\n",
       "\t\t\t</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to c:\\users\\dapeng\\appdata\\l\n",
      "[nltk_data]     ocal\\programs\\python\\python37\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run TextMiningModule.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**  \n",
    "<font color = blue, size = 4> \n",
    "    Your work will not be saved in Jupyter Notebook. You are recommended to copy your work and paste it to a safe place to record your work.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Download Your Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project will be much easier if you are able to download you work and save your progress. The link below will guide you to a resource which will provide instructions for setting up Jupyter Notebook on your own computer. This will allow you to download this notebook and save your work.\n",
    "\n",
    "<a href=\"https://datamine.unc.edu/wp-content/uploads/2020/06/FAQs.pdf\">Link to Instruction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to perform your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below the horizontal line for your writeup. Your writeup should describe your analytic process and your findings from your analysis. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
