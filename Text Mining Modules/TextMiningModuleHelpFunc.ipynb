{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(filename=\"./Static/Helpfunction.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # import \"re\" function\n",
    "import nltk # import nltk library\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import spacy                    #import spacy module\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case: \n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "  \n",
    "    return ' '.join(filtered_tokens)  \n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer() \n",
    "    \n",
    "    return ' '.join([ps.stem(word) for word in text.split()]) \n",
    "   \n",
    "def lemmatize_text(text):\n",
    "    s = \" \"\n",
    "    t_l = []\n",
    "    t_w = nltk.word_tokenize(text) \n",
    "    for w in t_w:\n",
    "        l_w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
    "        t_l.append(l_w)\n",
    "        \n",
    "    return s.join(t_l)  \n",
    "\n",
    "def expand_contractions(text):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = CONTRACTION_MAP.get(match)\\\n",
    "                                if CONTRACTION_MAP.get(match)\\\n",
    "                                else CONTRACTION_MAP.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "\n",
    "    return re.sub(\"'\", \"\", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ngram(document, N = 1, allgram = False): \n",
    "    \n",
    "    agg_words = ' '.join([text for text in document]) \n",
    "    tok_list = agg_words.split() \n",
    "    ngram_tok_list = [' '.join(toks) for toks in ngrams(tok_list, N)]\n",
    "    if allgram and N>1:\n",
    "        ngram = [ngrams(tok_list, i) for i in range(1,N)]\n",
    "        ngram_tok_list.extend(' '.join(toks) for ng in ngram for toks in ng)\n",
    "    fdist = nltk.FreqDist(ngram_tok_list) \n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()), \n",
    "                             'count':np.array(list(fdist.values())), \n",
    "                             'frequency': np.array(list(fdist.values()))/sum(fdist.values())}) \n",
    "    return words_df\n",
    "\n",
    "    \n",
    "def viz_ngram_freq(df, figsize = (8,8)):\n",
    "    plt.figure(figsize = figsize) \n",
    "    ax = sns.barplot(data=df, x= \"count\", y = \"word\") \n",
    "    ax.set(ylabel = 'Word') \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_matrix(document, tfidf = True):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(document) if tfidf else CountVectorizer(document)\n",
    "    bow_matrix = vectorizer.fit_transform(document)\n",
    "    df = pd.DataFrame(bow_matrix.toarray(), columns = vectorizer.get_feature_names())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_text_similarity(doc_a, doc_b, method = \"cosine\"):\n",
    "    \n",
    "    if isinstance(doc_a, str):\n",
    "        doc_a = [doc_a]\n",
    "    if isinstance(doc_b, str):\n",
    "        doc_b = [doc_b]\n",
    "    doc = doc_a + doc_b\n",
    "    bow_matrix = create_bow_matrix(doc)\n",
    "    sim_matrix = cosine_similarity(bow_matrix) if method == \"cosine\" else euclidean_distances(bow_matrix)\n",
    "    \n",
    "    return pd.DataFrame(sim_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtactInfo:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlpdoc = []\n",
    "\n",
    "    SUBJECTS_DEP = [\"nsubj\",  \"csubj\", \"expl\"]\n",
    "    PASSIVE_SUBJ_DEP = [\"nsubjpass\", \"csubjpass\"]\n",
    "    OBJECTS_DEP = [\"dobj\", \"dative\", \"pobj\", \"oprd\", ]\n",
    "    CONJ_DEP = [\"cc\", \"conj\"]\n",
    "    \n",
    "    def start_extract(self, document):\n",
    "        \n",
    "        if isinstance(document, str):\n",
    "            document = [document]        \n",
    "        \n",
    "        self.nlpdoc = list(nlp.pipe(document))\n",
    "\n",
    "    def get_postag(self, postagtype = \"univ\"):\n",
    "        \n",
    "        if len(self.nlpdoc) == 0:\n",
    "            raise ValueError((\"\"\"\n",
    "            The document is empty. \n",
    "            You should use start_extract() to initiate the extracter before you use this function\n",
    "            \"\"\"))\n",
    "\n",
    "        pos_tag = []\n",
    "        if postagtype == \"univ\":\n",
    "            for doc in self.nlpdoc:\n",
    "                pos_tag.append(dict(Counter([tok.pos_ for tok in doc])))\n",
    "        else:\n",
    "            for doc in self.nlpdoc:\n",
    "                pos_tag.append(dict(Counter([tok.tag_ for tok in doc])))            \n",
    "        \n",
    "        return pd.DataFrame(pos_tag, dtype='Int64').fillna(0)\n",
    "    \n",
    "    def get_noun_phrase(self):\n",
    "        \n",
    "        if len(self.nlpdoc) == 0:\n",
    "            raise ValueError((\"\"\"\n",
    "            The document is empty. \n",
    "            You should use start_extract() to initiate the extracter before you use this function\n",
    "            \"\"\"))\n",
    "\n",
    "        return [list(doc.noun_chunks) for doc in self.nlpdoc] \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
